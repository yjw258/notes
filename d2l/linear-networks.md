# 线性神经网络

## 1 线性回归

### 1.1 线性回归的基本元素

线性回归基于几个简单的假设：

* 首先，假设自变量 **x** 和因变量 **y** 之间的关系是线性的， 即 **y** 可以表示为 **x** 中元素的加权和，这里通常允许包含观测值的一些噪声

* 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布

为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为**训练数据集（training data set）** 或**训练集（training set）**。每行数据称为**样本（sample）**， 也可以称为**数据点（data point）**或**数据样本（data instance）**。 我们把试图预测的目标（比如预测房屋价格）称为**标签（label）**或**目标（target）**。 预测所依据的自变量（面积和房龄）称为**特征（feature）**或**协变量（covariate）**。

通常，我们使用 **n** 来表示数据集中的样本数。 对索引为 **i** 的样本，其输入表示为 $x^{(i)}=[x_1^{(i)},x_2^{(i)}]^T$ ， 其对应的标签是 $y^{(i)}$。

#### 1.1.1 线性模型

输入包含 **d** 个特征，将预测结果 $\hat y$ （通常使用“尖号”符号表示y的估计值）表示为：
$$
\hat y=w_1x_1+\cdots+w_dx+b
$$
将所有特征放到向量 $x\in R^d$ 中，并将所有权重放到向量 $2\in R^d$ 中，我们可以用点积形式来简洁地表达模型：
$$
\hat y=w^Tx+b
$$
用符号表示的矩阵 $X\in R^{n\times d}$ 可以很方便地引用我们整个数据集的n个样本。 其中，**X** 的每一行是一个样本，每一列是一种特征。对于特征集合 **X**，预测值 $\hat y\in R^n$ 可以通过矩阵-向量乘法表示为：
$$
\hat y=Xw+b
$$
在开始寻找最好的模型参数 **w** 和 b 之前，我们还需要：（1）一种模型质量的度量方式；（2）一种能够更新模型以提高模型预测质量的方法。

#### 1.1.2 损失函数

损失函数能够量化目标的 **实际值与预测值之间的差距** 。最常用的损失函数是平方误差函数。当样本 i 的预测值为 $\hat y^{(i)}$ ，其相应的真实标签为 $y^{(i)}$ ，平方误差定义为：
$$
l^{(i)}(w,b)=\frac{1}{2}(\hat y^{(i)}-y^{(i)})^2
$$
为了度量模型在整个数据集上的质量，我们需要计算在训练集n个样本上的损失均值：
$$
L(w,b)=\frac{1}{n}\sum_{i=1}^{n}l^{(i)}(w,b)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2
$$
在训练模型时，希望寻找一组参数 $(w^*,b^*)$ ，使得在所有训练样本上的总损失最小：
$$
w^*,b^*=argmin_{w,b}\ L(w,b)
$$

#### 1.1.3 解析解

线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解。

将偏置 b 合并到参数 **w** 中得到 **w'**，同时在样本矩阵中加一列1得到 **X'**，则预测问题变为最小化 $||y-Xw||^2$ 。令 $\frac{\partial ||y-Xw||^2}{\partial w}=0$，得到解析解 $w^*=(X^TX)^{-1}X^Ty$ 。