# 线性神经网络

## 1 线性回归

### 1.1 线性回归的基本元素

线性回归基于几个简单的假设：

* 首先，假设自变量 **x** 和因变量 **y** 之间的关系是线性的， 即 **y** 可以表示为 **x** 中元素的加权和，这里通常允许包含观测值的一些噪声

* 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布

为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为**训练数据集（training data set）** 或**训练集（training set）**。每行数据称为**样本（sample）**， 也可以称为**数据点（data point）**或**数据样本（data instance）**。 我们把试图预测的目标（比如预测房屋价格）称为**标签（label）**或**目标（target）**。 预测所依据的自变量（面积和房龄）称为**特征（feature）**或**协变量（covariate）**。

通常，我们使用 **n** 来表示数据集中的样本数。 对索引为 **i** 的样本，其输入表示为 $x^{(i)}=[x_1^{(i)},x_2^{(i)}]^T$ ， 其对应的标签是 $y^{(i)}$。

#### 1.1.1 线性模型

输入包含 **d** 个特征，将预测结果 $\hat y$ （通常使用“尖号”符号表示y的估计值）表示为：
$$
\hat y=w_1x_1+\cdots+w_dx+b
$$
将所有特征放到向量 $x\in R^d$ 中，并将所有权重放到向量 $2\in R^d$ 中，我们可以用点积形式来简洁地表达模型：
$$
\hat y=w^Tx+b
$$
用符号表示的矩阵 $X\in R^{n\times d}$ 可以很方便地引用我们整个数据集的n个样本。 其中，**X** 的每一行是一个样本，每一列是一种特征。对于特征集合 **X**，预测值 $\hat y\in R^n$ 可以通过矩阵-向量乘法表示为：
$$
\hat y=Xw+b
$$
在开始寻找最好的模型参数 **w** 和 b 之前，我们还需要：（1）一种模型质量的度量方式；（2）一种能够更新模型以提高模型预测质量的方法。

#### 1.1.2 损失函数

损失函数能够量化目标的 **实际值与预测值之间的差距** 。最常用的损失函数是平方误差函数。当样本 i 的预测值为 $\hat y^{(i)}$ ，其相应的真实标签为 $y^{(i)}$ ，平方误差定义为：
$$
l^{(i)}(w,b)=\frac{1}{2}(\hat y^{(i)}-y^{(i)})^2
$$
为了度量模型在整个数据集上的质量，我们需要计算在训练集n个样本上的损失均值：
$$
L(w,b)=\frac{1}{n}\sum_{i=1}^{n}l^{(i)}(w,b)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2
$$
在训练模型时，希望寻找一组参数 $(w^*,b^*)$ ，使得在所有训练样本上的总损失最小：
$$
w^*,b^*=argmin_{w,b}\ L(w,b)
$$

#### 1.1.3 解析解

线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解。

将偏置 b 合并到参数 **w** 中得到 **w'**，同时在样本矩阵中加一列1得到 **X'**，则预测问题变为最小化 $||y-Xw||^2$ 。令 $\frac{\partial ||y-Xw||^2}{\partial w}=0$，得到解析解 $w^*=(X^TX)^{-1}X^Ty$ 。

并不是所有问题都存在解析解，这导致它无法广泛应用在深度学习里。

#### 1.1.4 随机梯度下降

梯度下降的方法几乎可以优化所有深度学习模型，它通过不断地在损失函数递减的方向上更新参数来降低误差。最简单的用法是：计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数。但这样执行每一次更新参数前都要遍历整个数据集。因此，通常会在每次需要计算更新的时候抽取一小批样本，这种变体叫做 **小批量随机梯度下降** 。

随机抽样一个小批量 $\Beta$ ，学习率为 $\mu$ ，更新过程为：
$$
(w,b)\leftarrow (w,b)-\frac{\mu}{|\Beta|}\sum_{i\in \Beta}\partial_{(w,b)}l^{(i)}(w,b)
$$
批量大小 $|\Beta|$ 和学习率 $\mu$ 通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为 **超参数** 。**调参**是选择超参数的过程。

**泛化**：找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失。

#### 1.1.5 用模型进行预测

### 1.2 矢量化加速

与其在Python中编写开销高昂的for循环，对计算进行矢量化， 利用线性代数库，通常能带来数量级的加速。

### 1.3 正态分布与平方损失

正态分布（也叫高斯分布）：若随机变量 $x$ 具有均值 $\mu$ 和方差 $\sigma^2$ （标准差 $\sigma$ ），其正态分布概率密度函数：
$$
p(x)=\frac{1}{\sqrt{2\pi \sigma^2}}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}
$$

```python
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
```

均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。 噪声正态分布如下式：
$$
y=w^Tx+b+\epsilon
$$
其中， $\epsilon \sim N(0,\sigma^2)$ 。

### 1.4 从线性回归到深度网络

#### 1.4.1 神经网络图

![3.1.2](https://github.com/yjw258/notes/tree/main/d2l/figs/3.1.2.png)

#### 1.4.2 生物学